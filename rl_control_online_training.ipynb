{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Ops Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import subprocess\n",
    "import glob\n",
    "import skimage.io as imio\n",
    "import re\n",
    "from skimage.color import rgb2gray\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "Time = int(time.time()) % 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = '10.42.0.215'\n",
    "\n",
    "def initialFocus(sid=0, ip = IP):\n",
    "    cmD='ssh nvidia@'+ip+' \"echo \"nvidia\" | sudo -S ~/Documents/cam_on_Tegra/testCamera -id '+str(sid)+' -init \"'\n",
    "    pop=subprocess.run(cmD, stdout=subprocess.PIPE, shell=True)\n",
    "    return pop.stdout\n",
    "def moveFocus(sid=0, pos=512, ip = IP):\n",
    "    cmD='ssh nvidia@'+ip+' \"echo \"nvidia\" | sudo -S ~/Documents/cam_on_Tegra/testCamera -id '+str(sid)+' -mod '+str(pos)+'\"'\n",
    "    pop=subprocess.run(cmD, stdout=subprocess.PIPE, shell=True)\n",
    "    return pop.stdout\n",
    "def setFocus(sid=0, pos=512, ip = IP):\n",
    "    cmD='ssh nvidia@'+ip+' \"echo \"nvidia\" | sudo -S ~/Documents/cam_on_Tegra/testCamera -id '+str(sid)+' -set '+str(pos)+'\"'\n",
    "    pop=subprocess.run(cmD, stdout=subprocess.PIPE, shell=True)\n",
    "    return pop.stdout\n",
    "\n",
    "def grab_image(sid=0, imres=8, ip = IP):\n",
    "\n",
    "    subprocess.run('ssh nvidia@'+ip+' \"rm testGrab*\"', \\\n",
    "                   stdout=subprocess.PIPE, shell=True)\n",
    "    subprocess.run('rm testGrab*', \\\n",
    "                    stdout=subprocess.PIPE, shell=True)\n",
    "\n",
    "    subprocess.run('ssh nvidia@'+ip+' \"nvgstcapture-1.0 -m 1 --sensor-id '+str(sid)+' -A --capture-auto 1 \\\n",
    "                    --file-name testGrab --image-res '+str(imres)+'\"', stdout=subprocess.PIPE, shell=True)\n",
    "    subprocess.run('scp nvidia@'+ip+':testGrab* .', \\\n",
    "                   stdout=subprocess.PIPE, shell=True)\n",
    "    pop=subprocess.run('ls',stdout=subprocess.PIPE, shell=True)\n",
    "    rePop=pop.stdout\n",
    "    rePop=rePop.decode(\"utf-8\")\n",
    "    fileName = re.search(r'testGrab(.*).jpg', rePop)\n",
    "    fileName=fileName.group()\n",
    "    pop=imio.imread(fileName)\n",
    "    pop=skimage.transform.rotate(pop,180)\n",
    "    \n",
    "    return pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialFocus()\n",
    "curr = 800\n",
    "# setFocus(pos = curr)\n",
    "\n",
    "# img = grab_image()\n",
    "# gray_img = rgb2gray(img)\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autofocus Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pixel_estimator_with_weights()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quad_solver import solver\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "class pixel_estimator_with_weights(nn.Module):\n",
    "    def __init__(self, Weights,device = \"cuda:0\"):\n",
    "        ## Default: gpu mode\n",
    "        super(pixel_estimator_with_weights, self).__init__()\n",
    "        self.device = torch.device(device)\n",
    "        self.w1 = torch.from_numpy(Weights[0].transpose(3,2,0,1)).to(self.device)\n",
    "        self.b1 = torch.from_numpy(Weights[1]).to(self.device)\n",
    "        self.w2 = torch.tensor(Weights[2].transpose(3,2,0,1)).to(self.device)\n",
    "        self.b2 = torch.tensor(Weights[3]).to(self.device)\n",
    "        self.w3 = torch.tensor(Weights[4].transpose(3,2,0,1)).to(self.device)\n",
    "        self.b3 = torch.tensor(Weights[5]).to(self.device)\n",
    "        self.w4 = torch.tensor(Weights[6]).reshape(4,4,8,1024).permute(3,2,0,1).to(self.device)\n",
    "        self.b4 = torch.tensor(Weights[7]).to(self.device)\n",
    "        self.w5 = torch.tensor(Weights[8]).reshape(1,1,1024,512).permute(3,2,0,1).to(self.device)\n",
    "        self.b5 = torch.tensor(Weights[9]).to(self.device)\n",
    "        self.w6 = torch.tensor(Weights[10]).reshape(1,1,512,10).permute(3,2,0,1).to(self.device)\n",
    "        self.b6 = torch.tensor(Weights[11]).to(self.device)\n",
    "        self.w7 = torch.tensor(Weights[12]).reshape(1,1,10,1).permute(3,2,0,1).to(self.device)\n",
    "        self.b7 = torch.tensor(Weights[13]).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.conv2d(x,self.w1,bias = self.b1,stride=1))\n",
    "        x = F.relu(F.conv2d(x,self.w2,bias = self.b2,stride=1,dilation=8))\n",
    "        x = F.relu(F.conv2d(x,self.w3,bias = self.b3,stride=1,dilation=32))\n",
    "        x = F.leaky_relu(F.conv2d(x,self.w4,bias = self.b4,stride=8,dilation=128),0.1)\n",
    "        x = F.leaky_relu(F.conv2d(x,self.w5,bias = self.b5,stride=1),0.1)\n",
    "        x = F.leaky_relu(F.conv2d(x,self.w6,bias = self.b6,stride=1),0.1)\n",
    "        x = F.conv2d(x,self.w7,bias = self.b7,stride=1)\n",
    "        return x\n",
    "    \n",
    "model = torch.load('autofocus.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_patches(img, window= 1023, step = 512):\n",
    "    patches = []\n",
    "    H, W = img.shape\n",
    "    for i in range(0, H-step, step):\n",
    "        for j in range(0, W-step, step):\n",
    "            patches.append(img[i:i+window, j:j+window])\n",
    "    return np.stack(patches)\n",
    "\n",
    "def dist_est(img, last_dist_map = None, last_move_steps = None):\n",
    "    img = np.pad(img, ((200, 200), (128, 128)), 'reflect')\n",
    "#     plt.imshow(img)\n",
    "#     plt.show()\n",
    "    H, W = img.shape\n",
    "    \n",
    "    patches = crop_patches(img)\n",
    "    patches = torch.from_numpy(patches).float().unsqueeze(1)#.cuda()\n",
    "\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(patches.size(0)):\n",
    "            results.append(model(patches[i:i+1].cuda()).cpu())\n",
    "    results = torch.stack(results)\n",
    "\n",
    "    results = results.numpy()\n",
    "    results = results.squeeze()\n",
    "    \n",
    "    if last_dist_map is None:\n",
    "        last_dist_map = np.ones((H-512, W-512))\n",
    "\n",
    "    k = 0\n",
    "    n_img = np.zeros((H-512, W-512))\n",
    "    for i in range(0, H-512, 512):\n",
    "        for j in range(0, W-512, 512):\n",
    "            n_img[i:i+512, j:j+512] = cv2.resize(results[k], (0, 0), fx = 8, fy = 8)\n",
    "            k += 1\n",
    "    del results\n",
    "    n_img = np.clip(n_img, 0, 8)\n",
    "    \n",
    "    if last_move_steps is not None:\n",
    "        ## Focus direction: simple check\n",
    "        mapa = n_img - last_move_steps\n",
    "        mapb = -n_img - last_move_steps\n",
    "        diffa = np.abs(mapa) - last_dist_map\n",
    "        diffb = np.abs(mapb) - last_dist_map\n",
    "        mask = (np.abs(diffa) < np.abs(diffb)).astype(np.float64)\n",
    "        n_img = (n_img * mask + (1-mask) * (-n_img))\n",
    "\n",
    "    return n_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_img = dist_est(gray_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(n_img)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "# plt.hist(n_img.flatten())\n",
    "# print(n_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWnet module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from awnet import pwc_5x5_sigmoid_bilinear   # cm:import AWnet model\n",
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "AWnet = pwc_5x5_sigmoid_bilinear.pwc_residual().cuda()\n",
    "AWnet.load_state_dict(torch.load('awnet/fs_34_all_0.03036882.pkl'))\n",
    "AWnet = AWnet.eval()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fuseTwoImages(I, J_hat):\n",
    "    with torch.no_grad():\n",
    "        fusedTensor,warp,mask = AWnet(J_hat,I)\n",
    "    return fusedTensor#, warp, mask\n",
    "\n",
    "def patchize(img):\n",
    "    imgs = []\n",
    "    H, W, C = img.shape\n",
    "    ph = H//2\n",
    "    pw = W//2\n",
    "    img_empty = np.zeros((H+200, W+200, C))\n",
    "    img_empty[100:-100, 100:-100] = img\n",
    "    img = img_empty\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            imgs.append(img[100+ph*i-50:100+ph*i+ph+50, 100+pw*j-32:100+pw*j+pw+32])\n",
    "    imgs = np.stack(imgs)\n",
    "    return imgs\n",
    "\n",
    "def depatchize(imgs, pd_h = 50, pd_w = 32):\n",
    "    ph = (imgs[0].shape[0]-2*pd_h)\n",
    "    pw = (imgs[0].shape[1]-2*pd_w)\n",
    "    img = np.zeros((ph*2, pw*2, 3))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            img[i*ph:i*ph+ph, j*pw:j*pw+pw] = imgs[i*2+j, pd_h:-pd_h, pd_w:-pd_w]\n",
    "            \n",
    "    return img\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "def color_region(tss, locs):\n",
    "    tensors = tss.clone()\n",
    "    S, C, H, W = tensors.size()\n",
    "    assert S == locs.size(0)\n",
    "    if locs.size(1) == 2:\n",
    "        for i in range(S):\n",
    "            loc = locs[i]\n",
    "            window_size =  min(H, W)//4\n",
    "            x_l = int((loc[0]+1) * (H - window_size) / 2)\n",
    "            y_l = int((loc[1]+1) * (W - window_size) / 2)\n",
    "            x_r = int(min(H, x_l + window_size))\n",
    "            y_r = int(min(W, y_l + window_size))\n",
    "            tensors[i][1:, x_l:x_r, y_l:y_r] = -1\n",
    "            tensors[i][0, x_l:x_r, y_l:y_r] = 1\n",
    "            tensors[i][:, x_l+25:x_r-25, y_l+25:y_r-25] = tss[i][:, x_l+25:x_r-25, y_l+25:y_r-25]\n",
    "\n",
    "    return tensors\n",
    "    \n",
    "\n",
    "def image_fuse(a, b):\n",
    "    aa = patchize(a)\n",
    "    bb = patchize(b)\n",
    "    aa = torch.Tensor(aa.transpose(0, 3, 1, 2))\n",
    "    bb = torch.Tensor(bb.transpose(0, 3, 1, 2))\n",
    "\n",
    "    ccs = []\n",
    "    #wws = []\n",
    "    for i in range(4):\n",
    "        cc = fuseTwoImages(aa[i:i+1].cuda(), bb[i:i+1].cuda())\n",
    "        ccs.append(cc[0].cpu())\n",
    "        #wws.append(ww[0])\n",
    "    cc = torch.stack(ccs)\n",
    "    #ww = torch.stack(wws)\n",
    "\n",
    "    c = depatchize(cc.detach().numpy().transpose(0, 2, 3, 1))\n",
    "    del cc\n",
    "    #warp = depatchize(ww.cpu().detach().numpy().transpose(0, 2, 3, 1))\n",
    "    c = np.clip(c, 0, 1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus Ctrl Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 1.1160e-01,  6.8546e-02, -1.4885e-01,  3.7869e-02, -8.1315e-02,\n",
      "           -1.3020e-01,  1.1657e-01,  2.7113e-02],\n",
      "          [ 4.6420e-02,  1.0389e-01, -1.0570e-01,  2.6581e-01,  1.6375e-01,\n",
      "           -1.5427e-01,  6.6441e-02,  3.6922e-03],\n",
      "          [ 9.1292e-02, -5.8574e-02,  1.0795e-01, -9.9409e-02,  7.5464e-02,\n",
      "           -1.6159e-01,  1.1442e-01,  1.3031e-01],\n",
      "          [-5.2650e-03, -1.5881e-01,  1.2532e-01,  2.9367e-01, -3.7531e-01,\n",
      "           -1.6298e-01,  1.9903e-01,  2.1809e-01],\n",
      "          [-2.8191e-01, -1.6902e-01, -1.6099e-01, -1.7118e-01, -1.3073e-01,\n",
      "            6.9295e-02, -1.8010e-01, -4.4776e-02],\n",
      "          [-1.5061e-01, -1.0488e-01,  1.0380e-01, -1.8679e-01, -9.5900e-02,\n",
      "           -1.6959e-01, -8.9373e-02,  2.3692e-01],\n",
      "          [ 1.3213e-01, -6.7426e-02,  7.0138e-02, -1.0400e-01, -4.6078e-02,\n",
      "           -9.2193e-02, -2.5591e-01, -1.2325e-01],\n",
      "          [ 8.0314e-02, -5.6800e-03, -2.3350e-01, -8.4315e-02,  1.0001e-01,\n",
      "           -1.2966e-01, -5.9501e-01, -6.4450e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4673e-03,  1.9289e-01,  5.1943e-03, -4.2218e-02,  3.3692e-01,\n",
      "           -9.5307e-02, -1.3460e-01, -4.5059e-02],\n",
      "          [ 2.2473e-01, -5.4662e-02, -4.1362e-02, -5.1382e-04, -9.6039e-02,\n",
      "            1.0667e-01, -3.2006e-01,  2.3604e-01],\n",
      "          [-1.7712e-01, -4.2878e-02,  4.2794e-01, -1.8671e-03, -8.1894e-02,\n",
      "            1.5399e-01,  1.4710e-01, -1.6785e-01],\n",
      "          [ 1.1078e-01,  2.5241e-01,  1.4153e-01, -3.2329e-05,  1.1277e-01,\n",
      "            3.5214e-02,  9.7718e-02, -2.3084e-01],\n",
      "          [ 2.8135e-03, -2.3000e-01, -3.5976e-02,  1.3077e-01,  2.2409e-01,\n",
      "            9.6573e-02, -2.3329e-01,  1.9218e-01],\n",
      "          [ 4.8421e-01,  1.0657e-01,  4.7666e-02, -2.0110e-01,  1.9794e-01,\n",
      "            1.5933e-01, -1.7965e-02, -1.0946e-01],\n",
      "          [-1.4113e-01, -6.2365e-02, -3.4960e-01,  1.7999e-01,  1.1862e-01,\n",
      "            1.3081e-01, -7.1421e-02, -2.0622e-01],\n",
      "          [ 1.6642e-01,  1.7294e-01,  2.6523e-01,  1.2701e-01, -9.8119e-02,\n",
      "           -1.1881e-01, -1.8454e-01, -8.7882e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0803e-01,  2.4253e-01, -2.1229e-01, -6.9224e-02,  2.9058e-01,\n",
      "            1.7382e-02,  9.9893e-03,  2.0241e-01],\n",
      "          [ 7.5576e-02, -4.0428e-02, -1.7781e-02,  2.0813e-02, -2.9548e-01,\n",
      "           -1.2961e-01,  1.4702e-01,  8.7029e-03],\n",
      "          [-2.9236e-01,  2.3482e-02, -2.8883e-01,  7.8112e-02,  3.7774e-01,\n",
      "            1.2689e-02,  2.4816e-01,  1.1142e-01],\n",
      "          [-1.3204e-01, -1.2349e-01, -1.1744e-01,  5.7247e-02, -1.7585e-01,\n",
      "            9.5152e-02,  9.5436e-03,  1.6678e-01],\n",
      "          [-1.2302e-01,  1.1974e-01,  4.6829e-02, -2.0380e-01, -2.5628e-01,\n",
      "            6.2026e-02,  1.3838e-01, -1.3955e-01],\n",
      "          [-1.1223e-01, -2.1902e-01,  1.4476e-01,  4.9480e-02, -1.1024e-01,\n",
      "            1.8614e-01, -3.0279e-01, -2.8304e-01],\n",
      "          [ 8.3495e-03,  4.1621e-02,  2.3582e-02,  6.3352e-02, -1.1367e-01,\n",
      "           -1.1960e-01, -1.9500e-01,  1.8875e-01],\n",
      "          [ 1.9297e-01,  1.4468e-01,  1.2232e-01, -1.3876e-02, -4.3370e-02,\n",
      "           -1.6006e-01, -3.1420e-02,  5.8822e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0921e-01, -6.4314e-02, -2.4144e-01,  1.0637e-01, -9.0641e-02,\n",
      "           -2.0734e-01, -1.5455e-01, -1.5590e-01],\n",
      "          [ 4.6922e-02,  3.6465e-02, -1.8806e-01,  1.1101e-01, -1.7122e-01,\n",
      "           -2.4624e-01,  8.0323e-02, -1.2331e-01],\n",
      "          [-2.3734e-01,  1.1028e-01,  1.9288e-02,  2.4037e-02,  1.5942e-01,\n",
      "           -3.0903e-02,  7.6557e-02, -2.8559e-02],\n",
      "          [ 3.2204e-02, -2.6737e-02,  6.5025e-02, -1.6388e-01,  2.1668e-01,\n",
      "           -2.5125e-01,  1.6709e-01,  1.8798e-01],\n",
      "          [-2.3979e-01, -1.1967e-01,  1.9941e-01,  1.0296e-01,  8.0322e-02,\n",
      "           -3.4833e-01, -1.4703e-01, -1.4646e-01],\n",
      "          [-1.1482e-01, -1.5457e-01, -7.4952e-02,  1.5895e-02,  1.1466e-01,\n",
      "            1.9145e-02,  9.8099e-02,  5.0321e-01],\n",
      "          [ 3.9743e-01, -1.9854e-01, -4.2187e-01, -2.5822e-01, -8.0789e-02,\n",
      "            2.6292e-01, -1.9557e-01, -1.4713e-01],\n",
      "          [ 4.0698e-01,  6.5159e-02,  1.0267e-01, -1.2428e-02,  2.5715e-01,\n",
      "           -8.0865e-02, -4.8630e-02, -3.0253e-01]]]], requires_grad=True)\n",
      "[*] Loading model from ckpt/best_model\n",
      "current epoch: 288 --- best loss: 0.668776384197207\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from model import *\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from quad_solver import *\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_checkpoint(ckpt_path, model, optimizer):\n",
    "    \n",
    "    ckpt_dir = 'ckpt/'+ckpt_path\n",
    "\n",
    "    print(\"[*] Loading model from {}\".format(ckpt_dir))\n",
    "\n",
    "    ckpt_dir = '.'\n",
    "    filename = 'rfc_model_best_300.pth.tar'#'rfc_model_best_300.pth.tar' #\n",
    "    ckpt_path = os.path.join(ckpt_dir, filename)\n",
    "    ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "\n",
    "    # load variables from checkpoint\n",
    "    start_epoch = ckpt['epoch']\n",
    "    best_loss = ckpt['best_valid_mse']\n",
    "    print(\"current epoch: {} --- best loss: {}\".format(start_epoch, best_loss))\n",
    "    model.load_state_dict(ckpt['model_state'])\n",
    "    optimizer.load_state_dict(ckpt['optim_state'])   \n",
    "\n",
    "    return model, optimizer, start_epoch, best_loss\n",
    "\n",
    "\n",
    "def set_bn_eval(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.eval()\n",
    "\n",
    "rlmodel = focusLocNet(0.17, 1, 256, 2)#.to(\"cuda:0\")\n",
    "rlmodel.train()\n",
    "rlmodel.apply(set_bn_eval)\n",
    "for p in rlmodel.parameters():\n",
    "    print(p)\n",
    "    break\n",
    "# for name, param in rlmodel.named_parameters():\n",
    "#     if 'bn' in name:\n",
    "#         param.requires_grad = False\n",
    "optimizer= optim.Adam(filter(lambda p: p.requires_grad, rlmodel.parameters()), lr=1e-4)\n",
    "rlmodel, optimizer, epoch, best_loss = load_checkpoint('best_model', rlmodel, optimizer)\n",
    "# rlmodel = rlmodel.eval()\n",
    "\n",
    "def reset():\n",
    "    h = [torch.zeros(1, 1, 256),#.cuda(),\n",
    "                  torch.zeros(1, 1, 256)]#.cuda()]\n",
    "    l = torch.rand(1, 2)*2.0-1.0#.cuda() #-0.5~0.5\n",
    "    return h, l\n",
    "\n",
    "def dist_from_region(n_img, loc):\n",
    "    \n",
    "    assert len(n_img.shape) == 2\n",
    "\n",
    "    H, W = n_img.shape\n",
    "    window_size =  min(H, W)//4\n",
    "\n",
    "    x_l = int((loc[0]+1) * (H - window_size) / 2)\n",
    "    y_l = int((loc[1]+1) * (W - window_size) / 2)\n",
    "    x_r = int(min(H, x_l + window_size))\n",
    "    y_r = int(min(W, y_l + window_size))\n",
    "\n",
    "#     dist = -np.mean(n_img[x_l:x_r, y_l:y_r])\n",
    "    n, bins = np.histogram(n_img[x_l:x_r, y_l:y_r].flatten(), bins=10)\n",
    "#     n, bins, _ = plt.hist(n_img[x_l:x_r, y_l:y_r].flatten(), bins=10)\n",
    "#     plt.show()\n",
    "    idx_sorted = np.argsort(n)[::-1]\n",
    "    dist = -(bins[idx_sorted[0]]+bins[idx_sorted[0]+1])/2\n",
    "\n",
    "    return dist\n",
    "\n",
    "def greedyReward(input_t, locs):\n",
    "    batch_size, C, H, W = input_t.size()\n",
    "\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        loc = locs[i]\n",
    "        window_size = min(H, W)//4\n",
    "        x_l = int((loc[0]+1) * (H - window_size) / 2)\n",
    "        y_l = int((loc[1]+1) * (W - window_size) / 2)\n",
    "        x_r = int(min(H, x_l + window_size))\n",
    "        y_r = int(min(W, y_l + window_size))\n",
    "        if torch.mean(input_t[i][:, x_l:x_r, y_l:y_r]) > 0:\n",
    "            r = 1\n",
    "        else:\n",
    "            r = 0\n",
    "        rewards.append(r)\n",
    "    \n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    if input_t.is_cuda:\n",
    "        rewards = rewards.cuda()\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_img = None\n",
    "n_img = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "iii = 102\n",
    "        \n",
    "for epoch in range(1000):\n",
    "    print(\"epoch: \", epoch)\n",
    "    prev_img = None\n",
    "    n_img = None\n",
    "    h, l = reset()\n",
    "    curr = 800\n",
    "    log_pi = []\n",
    "    baselines = []\n",
    "    reward = []\n",
    "    \n",
    "    for kkk in range(2):\n",
    "    #     h, l = reset()\n",
    "        print(kkk, \"th caturing\")\n",
    "    #         img = cv2.imread('/home/qian/Documents/datasets/rfc_data/scene/scene1/101/800.jpg')[::-1, ::-1 ,::-1] / 255.0\n",
    "    #     img = cv2.imread('/home/qian/Documents/datasets/rfc_data/scene/scene1/{}/{}.jpg'.format(iii+kkk, curr))[::-1, ::-1 ,::-1] / 255.0\n",
    "        img = cv2.imread('/home/qian/Documents/datasets/rfc_data/scene/scene{}/{:03d}/{}.jpg'.format(iii//100, iii, curr))[::-1, ::-1 ,::-1] / 255.0\n",
    "\n",
    "    #         cv2.imwrite(\"rl_frames/oimg{:04d}_{:02d}.png\".format(Time, i), (img[...,::-1] * 255.0).astype(np.uint8))\n",
    "#         print(\"captured. Move now.\")\n",
    "\n",
    "\n",
    "\n",
    "        if prev_img is None:\n",
    "            prev_img = cv2.resize(img, None, fx = 0.5, fy = 0.5)\n",
    "        else:\n",
    "            ## AWnet 1920 x 1280\n",
    "            start_time = time.time()\n",
    "            prev_img = image_fuse(cv2.resize(img, None, fx = 0.5, fy = 0.5), prev_img)\n",
    "#             print(\"fuse time:\", time.time() - start_time)\n",
    "\n",
    "        gray_img = rgb2gray(img)\n",
    "        ## AF 3840 x 2160\n",
    "        if n_img is None:\n",
    "            n_img = dist_est(gray_img)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            n_img = dist_est(gray_img, np.abs(n_img), dist_to_move)\n",
    "#             print(\"dist_est time:\", time.time() - start_time)\n",
    "\n",
    "    #         cv2.imwrite(\"rl_frames/fimg{:04d}_{:02d}.png\".format(Time, i), (prev_img[...,::-1] * 255.0).astype(np.uint8))\n",
    "\n",
    "#         plt.imshow(prev_img)\n",
    "#         plt.show()\n",
    "\n",
    "        fused_nimg = dist_est(rgb2gray(cv2.resize(prev_img, None, fx = 2, fy = 2)))\n",
    "        n_img_resized = (cv2.resize(fused_nimg, (3072, 1536)) /8.0 * 255.0).astype(np.uint8)\n",
    "        input_t = torch.tensor(n_img_resized/127.5 - 1.0).float().unsqueeze(0).unsqueeze(0)\n",
    "        ## RL 128x64\n",
    "        start_time = time.time()\n",
    "\n",
    "        h, mu, l, b, p = rlmodel(input_t, l, h)#.cuda()\n",
    "        log_pi.append(p)\n",
    "        baselines.append(b)\n",
    "        r = greedyReward(input_t, l)#.cuda()\n",
    "        print(kkk, \"th reward: \", r.item())\n",
    "        reward.append(r)\n",
    "        for tt in range(kkk):\n",
    "            reward[tt] = reward[tt] + (0.9 ** (kkk - tt)) * r\n",
    "\n",
    "#         print(\"rl time:\", time.time() - start_time)\n",
    "        dist_to_move = dist_from_region(n_img, l.detach().squeeze().numpy())#.cpu()\n",
    "\n",
    "        input_show = torch.tensor(fused_nimg/8.0).float().unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        n_img_w_region = torchvision.utils.make_grid(color_region(input_show.repeat(1, 3, 1, 1), l.detach()))#.cpu()\n",
    "        imshow(n_img_w_region)\n",
    "    #         cv2.imwrite(\"rl_frames/nimg{:04d}_{:02d}.png\".format(Time, i), (n_img_w_region.numpy().transpose(1, 2, 0)[...,::-1] * 255.0).astype(np.uint8))    \n",
    "        del input_t\n",
    "        del n_img_w_region\n",
    "\n",
    "        curr = curr + solver(curr, dist_to_move)        \n",
    "        curr = np.clip(curr, 450, 1000)\n",
    "        if np.isnan(curr):\n",
    "            curr = 1000\n",
    "        curr = int(np.round(curr/50)*50)\n",
    "#         print(\"next curr: {}, dist to move: {:.1f}\".format(curr, dist_to_move))\n",
    "\n",
    "    baselines = torch.stack(baselines).transpose(1, 0)\n",
    "    log_pi = torch.stack(log_pi).transpose(1, 0)\n",
    "    R = torch.stack(reward).transpose(1, 0) * 1.0\n",
    "    print(\"total_reward:\", torch.mean(torch.sum(R, dim = 1),dim = 0).item())\n",
    "\n",
    "    loss_baseline = F.mse_loss(baselines, R)\n",
    "\n",
    "    adjusted_reward = R - baselines.detach()              \n",
    "\n",
    "    ## Basic REINFORCE algorithm\n",
    "    loss_reinforce = torch.sum(-log_pi*adjusted_reward, dim=1)\n",
    "    loss_reinforce = torch.mean(loss_reinforce, dim=0)\n",
    "\n",
    "    loss = loss_reinforce + loss_baseline\n",
    "\n",
    "    ##########---------------???????????????????------------------##########\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in rlmodel.parameters():\n",
    "    print(p)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_checkpoint(state):\n",
    "#     \"\"\"\n",
    "#     Save a copy of the model so that it can be loaded at a future\n",
    "#     date.\n",
    "#     \"\"\"\n",
    "#     filename = 'new_model_ckpt.pth.tar'\n",
    "#     torch.save(state, filename)\n",
    "\n",
    "        \n",
    "# save_checkpoint(\n",
    "#                 {'epoch': epoch + 1,\n",
    "#                  'model_state': rlmodel.state_dict(),\n",
    "#                  'optim_state': optimizer.state_dict(),\n",
    "#                  'best_valid_mse': best_loss,\n",
    "#                  }\n",
    "#             )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
